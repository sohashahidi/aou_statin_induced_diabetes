{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SID Genetics Study Step 2: Assigning Covariates for SID Phenotype\n",
    "\n",
    "## Objective\n",
    "The purpose of this notebook is to assign covariates to eligible statin users and non-users as part of the statin-induced diabetes (SID) phenotype. Covariates assigned include demographic characteristics (self-identified race, ethnicity, and sex at birth and age at index) and other baseline diabetes risk factors:\n",
    "- low high-density lipoprotein (HDL): HDL ≤40 for males and HDL ≤ 50 for females\n",
    "- high triglycerides (TG): TG ≥ 150\n",
    "- high body mass index (BMI): BMI ≥ 25\n",
    "- smoking status: has smoking observation in EHR\n",
    "- hypertension (HTN) status: has at least 2 HTN ICD codes to prevent misdiagnosis\n",
    "- prediabetes status: fasting glucose from 100-125mg/dL, random glucose from 140-199mg/dL, Hba1c between 5.7-6.4%\n",
    "- gestational diabetes status: has at least one ICD code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling in the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: The purpose of this section is to load packages and pull in data from the All of Us Research Project (AoURP). AoURP dataset code (R and SQL) is generated using the AoURP's cohort builder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Demographics (Sex & Race/Ethnicity) SQL\n",
    "\n",
    "library(tidyverse)\n",
    "library(bigrquery)\n",
    "\n",
    "# This query represents dataset \"All Subjects Covariate\" for domain \"person\" and was generated for All of Us Controlled Tier Dataset v7\n",
    "dataset_75324549_person_sql <- paste(\"\n",
    "    SELECT\n",
    "        person.person_id,\n",
    "        person.gender_concept_id,\n",
    "        p_gender_concept.concept_name as gender,\n",
    "        person.birth_datetime as date_of_birth,\n",
    "        person.race_concept_id,\n",
    "        p_race_concept.concept_name as race,\n",
    "        person.ethnicity_concept_id,\n",
    "        p_ethnicity_concept.concept_name as ethnicity,\n",
    "        person.sex_at_birth_concept_id,\n",
    "        p_sex_at_birth_concept.concept_name as sex_at_birth \n",
    "    FROM\n",
    "        `person` person \n",
    "    LEFT JOIN\n",
    "        `concept` p_gender_concept \n",
    "            ON person.gender_concept_id = p_gender_concept.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` p_race_concept \n",
    "            ON person.race_concept_id = p_race_concept.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` p_ethnicity_concept \n",
    "            ON person.ethnicity_concept_id = p_ethnicity_concept.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` p_sex_at_birth_concept \n",
    "            ON person.sex_at_birth_concept_id = p_sex_at_birth_concept.concept_id\", sep=\"\")\n",
    "\n",
    "# Formulate a Cloud Storage destination path for the data exported from BigQuery.\n",
    "# NOTE: By default data exported multiple times on the same day will overwrite older copies.\n",
    "#       But data exported on a different days will write to a new location so that historical\n",
    "#       copies can be kept as the dataset definition is changed.\n",
    "person_75324549_path <- file.path(\n",
    "  Sys.getenv(\"WORKSPACE_BUCKET\"),\n",
    "  \"bq_exports\",\n",
    "  Sys.getenv(\"OWNER_EMAIL\"),\n",
    "  strftime(lubridate::now(), \"%Y%m%d\"),  # Comment out this line if you want the export to always overwrite.\n",
    "  \"person_75324549\",\n",
    "  \"person_75324549_*.csv\")\n",
    "message(str_glue('The data will be written to {person_75324549_path}. Use this path when reading ',\n",
    "                 'the data into your notebooks in the future.'))\n",
    "\n",
    "# Perform the query and export the dataset to Cloud Storage as CSV files.\n",
    "# NOTE: You only need to run `bq_table_save` once. After that, you can\n",
    "#       just read data from the CSVs in Cloud Storage.\n",
    "bq_table_save(\n",
    "  bq_dataset_query(Sys.getenv(\"WORKSPACE_CDR\"), dataset_75324549_person_sql, billing = Sys.getenv(\"GOOGLE_PROJECT\")),\n",
    "  person_75324549_path,\n",
    "  destination_format = \"CSV\")\n",
    "\n",
    "# Read the data directly from Cloud Storage into memory.\n",
    "# NOTE: Alternatively you can `gsutil -m cp {person_75324549_path}` to copy these files\n",
    "#       to the Jupyter disk.\n",
    "read_bq_export_from_workspace_bucket <- function(export_path) {\n",
    "  col_types <- cols(gender = col_character(), race = col_character(), ethnicity = col_character(), sex_at_birth = col_character())\n",
    "  bind_rows(\n",
    "    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),\n",
    "        function(csv) {\n",
    "          message(str_glue('Loading {csv}.'))\n",
    "          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "          if (is.null(col_types)) {\n",
    "            col_types <- spec(chunk)\n",
    "          }\n",
    "          chunk\n",
    "        }))\n",
    "}\n",
    "dataset_75324549_person_df <- read_bq_export_from_workspace_bucket(person_75324549_path)\n",
    "\n",
    "dim(dataset_75324549_person_df)\n",
    "\n",
    "# head(dataset_75324549_person_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Gestational Diabetes, Prediabetes, HTN SQL - dataset_75324549_condition_df\n",
    "\n",
    "library(tidyverse)\n",
    "library(bigrquery)\n",
    "\n",
    "# This query represents dataset \"All Subjects Covariate\" for domain \"condition\" and was generated for All of Us Controlled Tier Dataset v7\n",
    "dataset_75324549_condition_sql <- paste(\"\n",
    "    SELECT\n",
    "        c_occurrence.person_id,\n",
    "        c_occurrence.condition_concept_id,\n",
    "        c_standard_concept.concept_name as standard_concept_name,\n",
    "        c_standard_concept.concept_code as standard_concept_code,\n",
    "        c_standard_concept.vocabulary_id as standard_vocabulary,\n",
    "        c_occurrence.condition_start_datetime,\n",
    "        c_occurrence.condition_end_datetime,\n",
    "        c_occurrence.condition_type_concept_id,\n",
    "        c_type.concept_name as condition_type_concept_name,\n",
    "        c_occurrence.stop_reason,\n",
    "        c_occurrence.visit_occurrence_id,\n",
    "        visit.concept_name as visit_occurrence_concept_name,\n",
    "        c_occurrence.condition_source_value,\n",
    "        c_occurrence.condition_source_concept_id,\n",
    "        c_source_concept.concept_name as source_concept_name,\n",
    "        c_source_concept.concept_code as source_concept_code,\n",
    "        c_source_concept.vocabulary_id as source_vocabulary,\n",
    "        c_occurrence.condition_status_source_value,\n",
    "        c_occurrence.condition_status_concept_id,\n",
    "        c_status.concept_name as condition_status_concept_name \n",
    "    FROM\n",
    "        ( SELECT\n",
    "            * \n",
    "        FROM\n",
    "            `condition_occurrence` c_occurrence \n",
    "        WHERE\n",
    "            (\n",
    "                condition_source_concept_id IN (SELECT\n",
    "                    DISTINCT c.concept_id \n",
    "                FROM\n",
    "                    `cb_criteria` c \n",
    "                JOIN\n",
    "                    (SELECT\n",
    "                        CAST(cr.id as string) AS id       \n",
    "                    FROM\n",
    "                        `cb_criteria` cr       \n",
    "                    WHERE\n",
    "                        concept_id IN (1571690, 1571691, 35207668, 37200977, 37200978, 37200979, 37201113, 44821949, 44822099, 44822104, 44823109, 44823246, 44823247, 44829117, 44830221, 44831389, 44831390, 44832532, 44832533, 44833556, 44834715, 44836084, 44837245, 45539106, 45553483, 45558215, 45563059, 45572770, 45582459, 45582460, 45582461, 45592198)       \n",
    "                        AND full_text LIKE '%_rank1]%'      ) a \n",
    "                        ON (c.path LIKE CONCAT('%.', a.id, '.%') \n",
    "                        OR c.path LIKE CONCAT('%.', a.id) \n",
    "                        OR c.path LIKE CONCAT(a.id, '.%') \n",
    "                        OR c.path = a.id) \n",
    "                WHERE\n",
    "                    is_standard = 0 \n",
    "                    AND is_selectable = 1)\n",
    "            )) c_occurrence \n",
    "    LEFT JOIN\n",
    "        `concept` c_standard_concept \n",
    "            ON c_occurrence.condition_concept_id = c_standard_concept.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` c_type \n",
    "            ON c_occurrence.condition_type_concept_id = c_type.concept_id \n",
    "    LEFT JOIN\n",
    "        `visit_occurrence` v \n",
    "            ON c_occurrence.visit_occurrence_id = v.visit_occurrence_id \n",
    "    LEFT JOIN\n",
    "        `concept` visit \n",
    "            ON v.visit_concept_id = visit.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` c_source_concept \n",
    "            ON c_occurrence.condition_source_concept_id = c_source_concept.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` c_status \n",
    "            ON c_occurrence.condition_status_concept_id = c_status.concept_id\", sep=\"\")\n",
    "\n",
    "# Formulate a Cloud Storage destination path for the data exported from BigQuery.\n",
    "# NOTE: By default data exported multiple times on the same day will overwrite older copies.\n",
    "#       But data exported on a different days will write to a new location so that historical\n",
    "#       copies can be kept as the dataset definition is changed.\n",
    "condition_75324549_path <- file.path(\n",
    "  Sys.getenv(\"WORKSPACE_BUCKET\"),\n",
    "  \"bq_exports\",\n",
    "  Sys.getenv(\"OWNER_EMAIL\"),\n",
    "  strftime(lubridate::now(), \"%Y%m%d\"),  # Comment out this line if you want the export to always overwrite.\n",
    "  \"condition_75324549\",\n",
    "  \"condition_75324549_*.csv\")\n",
    "message(str_glue('The data will be written to {condition_75324549_path}. Use this path when reading ',\n",
    "                 'the data into your notebooks in the future.'))\n",
    "\n",
    "# Perform the query and export the dataset to Cloud Storage as CSV files.\n",
    "# NOTE: You only need to run `bq_table_save` once. After that, you can\n",
    "#       just read data from the CSVs in Cloud Storage.\n",
    "bq_table_save(\n",
    "  bq_dataset_query(Sys.getenv(\"WORKSPACE_CDR\"), dataset_75324549_condition_sql, billing = Sys.getenv(\"GOOGLE_PROJECT\")),\n",
    "  condition_75324549_path,\n",
    "  destination_format = \"CSV\")\n",
    "\n",
    "# Read the data directly from Cloud Storage into memory.\n",
    "# NOTE: Alternatively you can `gsutil -m cp {condition_75324549_path}` to copy these files\n",
    "#       to the Jupyter disk.\n",
    "read_bq_export_from_workspace_bucket <- function(export_path) {\n",
    "  col_types <- cols(standard_concept_name = col_character(), standard_concept_code = col_character(), standard_vocabulary = col_character(), condition_type_concept_name = col_character(), stop_reason = col_character(), visit_occurrence_concept_name = col_character(), condition_source_value = col_character(), source_concept_name = col_character(), source_concept_code = col_character(), source_vocabulary = col_character(), condition_status_source_value = col_character(), condition_status_concept_name = col_character())\n",
    "  bind_rows(\n",
    "    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),\n",
    "        function(csv) {\n",
    "          message(str_glue('Loading {csv}.'))\n",
    "          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "          if (is.null(col_types)) {\n",
    "            col_types <- spec(chunk)\n",
    "          }\n",
    "          chunk\n",
    "        }))\n",
    "}\n",
    "dataset_75324549_condition_df <- read_bq_export_from_workspace_bucket(condition_75324549_path)\n",
    "\n",
    "dim(dataset_75324549_condition_df)\n",
    "\n",
    "# head(dataset_75324549_condition_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Filter HTN data frame to make sure patients are only diagnosed with HTN if they have at least 2 ICD codes\n",
    "htn_icd_df <- dataset_75324549_condition_df[which(grepl(pattern = \"I10*|401*\", \n",
    "                                                        dataset_75324549_condition_df$source_concept_code)),\n",
    "                                           c('person_id', 'standard_concept_name', 'condition_start_datetime', \n",
    "                                             'source_concept_code')]\n",
    "\n",
    "htn_diagnosis <- data.frame(table(htn_icd_df$person_id))\n",
    "htn_ids <- htn_diagnosis[htn_diagnosis$Freq > 1,]\n",
    "\n",
    "htn_icd_filtered_df <- htn_icd_df %>% filter(person_id %in% htn_ids$Var1) %>% \n",
    "group_by(person_id) %>% \n",
    "arrange(as.Date(condition_start_datetime)) %>% slice_head()\n",
    "\n",
    "dim(htn_icd_filtered_df)\n",
    "# head(htn_icd_filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Get first GD only\n",
    "gd_code_df <- dataset_75324549_condition_df %>% filter(grepl(pattern = '648\\\\.8.|648\\\\.0.|O24\\\\.4.', \n",
    "                                                             condition_source_value )) %>% \n",
    "                group_by(person_id) %>% \n",
    "                arrange(condition_start_datetime) %>% \n",
    "                slice_head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# BMI measurements SQL - dataset_33462820_measurement_df\n",
    "\n",
    "library(tidyverse)\n",
    "library(bigrquery)\n",
    "\n",
    "# This query represents dataset \"BMI Dataset\" for domain \"measurement\" and was generated for All of Us Controlled Tier Dataset v7\n",
    "dataset_33462820_measurement_sql <- paste(\"\n",
    "    SELECT\n",
    "        measurement.person_id,\n",
    "        measurement.measurement_concept_id,\n",
    "        m_standard_concept.concept_name as standard_concept_name,\n",
    "        m_standard_concept.concept_code as standard_concept_code,\n",
    "        m_standard_concept.vocabulary_id as standard_vocabulary,\n",
    "        measurement.measurement_datetime,\n",
    "        measurement.measurement_type_concept_id,\n",
    "        m_type.concept_name as measurement_type_concept_name,\n",
    "        measurement.operator_concept_id,\n",
    "        m_operator.concept_name as operator_concept_name,\n",
    "        measurement.value_as_number,\n",
    "        measurement.value_as_concept_id,\n",
    "        m_value.concept_name as value_as_concept_name,\n",
    "        measurement.unit_concept_id,\n",
    "        m_unit.concept_name as unit_concept_name,\n",
    "        measurement.range_low,\n",
    "        measurement.range_high,\n",
    "        measurement.visit_occurrence_id,\n",
    "        m_visit.concept_name as visit_occurrence_concept_name,\n",
    "        measurement.measurement_source_value,\n",
    "        measurement.measurement_source_concept_id,\n",
    "        m_source_concept.concept_name as source_concept_name,\n",
    "        m_source_concept.concept_code as source_concept_code,\n",
    "        m_source_concept.vocabulary_id as source_vocabulary,\n",
    "        measurement.unit_source_value,\n",
    "        measurement.value_source_value \n",
    "    FROM\n",
    "        ( SELECT\n",
    "            * \n",
    "        FROM\n",
    "            `measurement` measurement \n",
    "        WHERE\n",
    "            (\n",
    "                measurement_concept_id IN (SELECT\n",
    "                    DISTINCT c.concept_id \n",
    "                FROM\n",
    "                    `cb_criteria` c \n",
    "                JOIN\n",
    "                    (SELECT\n",
    "                        CAST(cr.id as string) AS id       \n",
    "                    FROM\n",
    "                        `cb_criteria` cr       \n",
    "                    WHERE\n",
    "                        concept_id IN (3038553)       \n",
    "                        AND full_text LIKE '%_rank1]%'      ) a \n",
    "                        ON (c.path LIKE CONCAT('%.', a.id, '.%') \n",
    "                        OR c.path LIKE CONCAT('%.', a.id) \n",
    "                        OR c.path LIKE CONCAT(a.id, '.%') \n",
    "                        OR c.path = a.id) \n",
    "                WHERE\n",
    "                    is_standard = 1 \n",
    "                    AND is_selectable = 1)\n",
    "            )) measurement \n",
    "    LEFT JOIN\n",
    "        `concept` m_standard_concept \n",
    "            ON measurement.measurement_concept_id = m_standard_concept.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_type \n",
    "            ON measurement.measurement_type_concept_id = m_type.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_operator \n",
    "            ON measurement.operator_concept_id = m_operator.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_value \n",
    "            ON measurement.value_as_concept_id = m_value.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_unit \n",
    "            ON measurement.unit_concept_id = m_unit.concept_id \n",
    "    LEFT JOIn\n",
    "        `visit_occurrence` v \n",
    "            ON measurement.visit_occurrence_id = v.visit_occurrence_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_visit \n",
    "            ON v.visit_concept_id = m_visit.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_source_concept \n",
    "            ON measurement.measurement_source_concept_id = m_source_concept.concept_id\", sep=\"\")\n",
    "\n",
    "# Formulate a Cloud Storage destination path for the data exported from BigQuery.\n",
    "# NOTE: By default data exported multiple times on the same day will overwrite older copies.\n",
    "#       But data exported on a different days will write to a new location so that historical\n",
    "#       copies can be kept as the dataset definition is changed.\n",
    "measurement_33462820_path <- file.path(\n",
    "  Sys.getenv(\"WORKSPACE_BUCKET\"),\n",
    "  \"bq_exports\",\n",
    "  Sys.getenv(\"OWNER_EMAIL\"),\n",
    "  strftime(lubridate::now(), \"%Y%m%d\"),  # Comment out this line if you want the export to always overwrite.\n",
    "  \"measurement_33462820\",\n",
    "  \"measurement_33462820_*.csv\")\n",
    "message(str_glue('The data will be written to {measurement_33462820_path}. Use this path when reading ',\n",
    "                 'the data into your notebooks in the future.'))\n",
    "\n",
    "# Perform the query and export the dataset to Cloud Storage as CSV files.\n",
    "# NOTE: You only need to run `bq_table_save` once. After that, you can\n",
    "#       just read data from the CSVs in Cloud Storage.\n",
    "bq_table_save(\n",
    "  bq_dataset_query(Sys.getenv(\"WORKSPACE_CDR\"), dataset_33462820_measurement_sql, billing = Sys.getenv(\"GOOGLE_PROJECT\")),\n",
    "  measurement_33462820_path,\n",
    "  destination_format = \"CSV\")\n",
    "\n",
    "\n",
    "# Read the data directly from Cloud Storage into memory.\n",
    "# NOTE: Alternatively you can `gsutil -m cp {measurement_33462820_path}` to copy these files\n",
    "#       to the Jupyter disk.\n",
    "read_bq_export_from_workspace_bucket <- function(export_path) {\n",
    "  col_types <- cols(standard_concept_name = col_character(), standard_concept_code = col_character(), standard_vocabulary = col_character(), measurement_type_concept_name = col_character(), operator_concept_name = col_character(), value_as_concept_name = col_character(), unit_concept_name = col_character(), visit_occurrence_concept_name = col_character(), measurement_source_value = col_character(), source_concept_name = col_character(), source_concept_code = col_character(), source_vocabulary = col_character(), unit_source_value = col_character(), value_source_value = col_character())\n",
    "  bind_rows(\n",
    "    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),\n",
    "        function(csv) {\n",
    "          message(str_glue('Loading {csv}.'))\n",
    "          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "          if (is.null(col_types)) {\n",
    "            col_types <- spec(chunk)\n",
    "          }\n",
    "          chunk\n",
    "        }))\n",
    "}\n",
    "dataset_33462820_measurement_df <- read_bq_export_from_workspace_bucket(measurement_33462820_path)\n",
    "\n",
    "dim(dataset_33462820_measurement_df)\n",
    "\n",
    "# head(dataset_33462820_measurement_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# HDL Measurements SQL - dataset_21028286_measurement_df\n",
    "library(tidyverse)\n",
    "library(bigrquery)\n",
    "\n",
    "# This query represents dataset \"HDL Measures Dataset\" for domain \"measurement\" and was generated for All of Us Controlled Tier Dataset v7\n",
    "dataset_21028286_measurement_sql <- paste(\"\n",
    "    SELECT\n",
    "        measurement.person_id,\n",
    "        measurement.measurement_concept_id,\n",
    "        m_standard_concept.concept_name as standard_concept_name,\n",
    "        m_standard_concept.concept_code as standard_concept_code,\n",
    "        m_standard_concept.vocabulary_id as standard_vocabulary,\n",
    "        measurement.measurement_datetime,\n",
    "        measurement.measurement_type_concept_id,\n",
    "        m_type.concept_name as measurement_type_concept_name,\n",
    "        measurement.operator_concept_id,\n",
    "        m_operator.concept_name as operator_concept_name,\n",
    "        measurement.value_as_number,\n",
    "        measurement.value_as_concept_id,\n",
    "        m_value.concept_name as value_as_concept_name,\n",
    "        measurement.unit_concept_id,\n",
    "        m_unit.concept_name as unit_concept_name,\n",
    "        measurement.range_low,\n",
    "        measurement.range_high,\n",
    "        measurement.visit_occurrence_id,\n",
    "        m_visit.concept_name as visit_occurrence_concept_name,\n",
    "        measurement.measurement_source_value,\n",
    "        measurement.measurement_source_concept_id,\n",
    "        m_source_concept.concept_name as source_concept_name,\n",
    "        m_source_concept.concept_code as source_concept_code,\n",
    "        m_source_concept.vocabulary_id as source_vocabulary,\n",
    "        measurement.unit_source_value,\n",
    "        measurement.value_source_value \n",
    "    FROM\n",
    "        ( SELECT\n",
    "            * \n",
    "        FROM\n",
    "            `measurement` measurement \n",
    "        WHERE\n",
    "            (\n",
    "                measurement_concept_id IN (SELECT\n",
    "                    DISTINCT c.concept_id \n",
    "                FROM\n",
    "                    `cb_criteria` c \n",
    "                JOIN\n",
    "                    (SELECT\n",
    "                        CAST(cr.id as string) AS id       \n",
    "                    FROM\n",
    "                        `cb_criteria` cr       \n",
    "                    WHERE\n",
    "                        concept_id IN (40782589)       \n",
    "                        AND full_text LIKE '%_rank1]%'      ) a \n",
    "                        ON (c.path LIKE CONCAT('%.', a.id, '.%') \n",
    "                        OR c.path LIKE CONCAT('%.', a.id) \n",
    "                        OR c.path LIKE CONCAT(a.id, '.%') \n",
    "                        OR c.path = a.id) \n",
    "                WHERE\n",
    "                    is_standard = 1 \n",
    "                    AND is_selectable = 1)\n",
    "            )) measurement \n",
    "    LEFT JOIN\n",
    "        `concept` m_standard_concept \n",
    "            ON measurement.measurement_concept_id = m_standard_concept.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_type \n",
    "            ON measurement.measurement_type_concept_id = m_type.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_operator \n",
    "            ON measurement.operator_concept_id = m_operator.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_value \n",
    "            ON measurement.value_as_concept_id = m_value.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_unit \n",
    "            ON measurement.unit_concept_id = m_unit.concept_id \n",
    "    LEFT JOIn\n",
    "        `visit_occurrence` v \n",
    "            ON measurement.visit_occurrence_id = v.visit_occurrence_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_visit \n",
    "            ON v.visit_concept_id = m_visit.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_source_concept \n",
    "            ON measurement.measurement_source_concept_id = m_source_concept.concept_id\", sep=\"\")\n",
    "\n",
    "# Formulate a Cloud Storage destination path for the data exported from BigQuery.\n",
    "# NOTE: By default data exported multiple times on the same day will overwrite older copies.\n",
    "#       But data exported on a different days will write to a new location so that historical\n",
    "#       copies can be kept as the dataset definition is changed.\n",
    "measurement_21028286_path <- file.path(\n",
    "  Sys.getenv(\"WORKSPACE_BUCKET\"),\n",
    "  \"bq_exports\",\n",
    "  Sys.getenv(\"OWNER_EMAIL\"),\n",
    "  strftime(lubridate::now(), \"%Y%m%d\"),  # Comment out this line if you want the export to always overwrite.\n",
    "  \"measurement_21028286\",\n",
    "  \"measurement_21028286_*.csv\")\n",
    "message(str_glue('The data will be written to {measurement_21028286_path}. Use this path when reading ',\n",
    "                 'the data into your notebooks in the future.'))\n",
    "\n",
    "# Perform the query and export the dataset to Cloud Storage as CSV files.\n",
    "# NOTE: You only need to run `bq_table_save` once. After that, you can\n",
    "#       just read data from the CSVs in Cloud Storage.\n",
    "bq_table_save(\n",
    "  bq_dataset_query(Sys.getenv(\"WORKSPACE_CDR\"), dataset_21028286_measurement_sql, billing = Sys.getenv(\"GOOGLE_PROJECT\")),\n",
    "  measurement_21028286_path,\n",
    "  destination_format = \"CSV\")\n",
    "\n",
    "\n",
    "# Read the data directly from Cloud Storage into memory.\n",
    "# NOTE: Alternatively you can `gsutil -m cp {measurement_21028286_path}` to copy these files\n",
    "#       to the Jupyter disk.\n",
    "read_bq_export_from_workspace_bucket <- function(export_path) {\n",
    "  col_types <- cols(standard_concept_name = col_character(), standard_concept_code = col_character(), standard_vocabulary = col_character(), measurement_type_concept_name = col_character(), operator_concept_name = col_character(), value_as_concept_name = col_character(), unit_concept_name = col_character(), visit_occurrence_concept_name = col_character(), measurement_source_value = col_character(), source_concept_name = col_character(), source_concept_code = col_character(), source_vocabulary = col_character(), unit_source_value = col_character(), value_source_value = col_character())\n",
    "  bind_rows(\n",
    "    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),\n",
    "        function(csv) {\n",
    "          message(str_glue('Loading {csv}.'))\n",
    "          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "          if (is.null(col_types)) {\n",
    "            col_types <- spec(chunk)\n",
    "          }\n",
    "          chunk\n",
    "        }))\n",
    "}\n",
    "dataset_21028286_measurement_df <- read_bq_export_from_workspace_bucket(measurement_21028286_path)\n",
    "\n",
    "dim(dataset_21028286_measurement_df)\n",
    "\n",
    "# head(dataset_21028286_measurement_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# LDL cholesterol measures - dataset_69758583_measurement_df\n",
    "\n",
    "library(tidyverse)\n",
    "library(bigrquery)\n",
    "\n",
    "# This query represents dataset \"LDL Measures\" for domain \"measurement\" and was generated for All of Us Controlled Tier Dataset v7\n",
    "dataset_69758583_measurement_sql <- paste(\"\n",
    "    SELECT\n",
    "        measurement.person_id,\n",
    "        measurement.measurement_concept_id,\n",
    "        m_standard_concept.concept_name as standard_concept_name,\n",
    "        m_standard_concept.concept_code as standard_concept_code,\n",
    "        m_standard_concept.vocabulary_id as standard_vocabulary,\n",
    "        measurement.measurement_datetime,\n",
    "        measurement.measurement_type_concept_id,\n",
    "        m_type.concept_name as measurement_type_concept_name,\n",
    "        measurement.operator_concept_id,\n",
    "        m_operator.concept_name as operator_concept_name,\n",
    "        measurement.value_as_number,\n",
    "        measurement.value_as_concept_id,\n",
    "        m_value.concept_name as value_as_concept_name,\n",
    "        measurement.unit_concept_id,\n",
    "        m_unit.concept_name as unit_concept_name,\n",
    "        measurement.range_low,\n",
    "        measurement.range_high,\n",
    "        measurement.visit_occurrence_id,\n",
    "        m_visit.concept_name as visit_occurrence_concept_name,\n",
    "        measurement.measurement_source_value,\n",
    "        measurement.measurement_source_concept_id,\n",
    "        m_source_concept.concept_name as source_concept_name,\n",
    "        m_source_concept.concept_code as source_concept_code,\n",
    "        m_source_concept.vocabulary_id as source_vocabulary,\n",
    "        measurement.unit_source_value,\n",
    "        measurement.value_source_value \n",
    "    FROM\n",
    "        ( SELECT\n",
    "            * \n",
    "        FROM\n",
    "            `measurement` measurement \n",
    "        WHERE\n",
    "            (\n",
    "                measurement_concept_id IN (SELECT\n",
    "                    DISTINCT c.concept_id \n",
    "                FROM\n",
    "                    `cb_criteria` c \n",
    "                JOIN\n",
    "                    (SELECT\n",
    "                        CAST(cr.id as string) AS id       \n",
    "                    FROM\n",
    "                        `cb_criteria` cr       \n",
    "                    WHERE\n",
    "                        concept_id IN (40795800)       \n",
    "                        AND full_text LIKE '%_rank1]%'      ) a \n",
    "                        ON (c.path LIKE CONCAT('%.', a.id, '.%') \n",
    "                        OR c.path LIKE CONCAT('%.', a.id) \n",
    "                        OR c.path LIKE CONCAT(a.id, '.%') \n",
    "                        OR c.path = a.id) \n",
    "                WHERE\n",
    "                    is_standard = 1 \n",
    "                    AND is_selectable = 1)\n",
    "            )) measurement \n",
    "    LEFT JOIN\n",
    "        `concept` m_standard_concept \n",
    "            ON measurement.measurement_concept_id = m_standard_concept.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_type \n",
    "            ON measurement.measurement_type_concept_id = m_type.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_operator \n",
    "            ON measurement.operator_concept_id = m_operator.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_value \n",
    "            ON measurement.value_as_concept_id = m_value.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_unit \n",
    "            ON measurement.unit_concept_id = m_unit.concept_id \n",
    "    LEFT JOIn\n",
    "        `visit_occurrence` v \n",
    "            ON measurement.visit_occurrence_id = v.visit_occurrence_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_visit \n",
    "            ON v.visit_concept_id = m_visit.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_source_concept \n",
    "            ON measurement.measurement_source_concept_id = m_source_concept.concept_id\", sep=\"\")\n",
    "\n",
    "# Formulate a Cloud Storage destination path for the data exported from BigQuery.\n",
    "# NOTE: By default data exported multiple times on the same day will overwrite older copies.\n",
    "#       But data exported on a different days will write to a new location so that historical\n",
    "#       copies can be kept as the dataset definition is changed.\n",
    "measurement_69758583_path <- file.path(\n",
    "  Sys.getenv(\"WORKSPACE_BUCKET\"),\n",
    "  \"bq_exports\",\n",
    "  Sys.getenv(\"OWNER_EMAIL\"),\n",
    "  strftime(lubridate::now(), \"%Y%m%d\"),  # Comment out this line if you want the export to always overwrite.\n",
    "  \"measurement_69758583\",\n",
    "  \"measurement_69758583_*.csv\")\n",
    "message(str_glue('The data will be written to {measurement_69758583_path}. Use this path when reading ',\n",
    "                 'the data into your notebooks in the future.'))\n",
    "\n",
    "# Perform the query and export the dataset to Cloud Storage as CSV files.\n",
    "# NOTE: You only need to run `bq_table_save` once. After that, you can\n",
    "#       just read data from the CSVs in Cloud Storage.\n",
    "bq_table_save(\n",
    "  bq_dataset_query(Sys.getenv(\"WORKSPACE_CDR\"), dataset_69758583_measurement_sql, billing = Sys.getenv(\"GOOGLE_PROJECT\")),\n",
    "  measurement_69758583_path,\n",
    "  destination_format = \"CSV\")\n",
    "\n",
    "\n",
    "# Read the data directly from Cloud Storage into memory.\n",
    "# NOTE: Alternatively you can `gsutil -m cp {measurement_69758583_path}` to copy these files\n",
    "#       to the Jupyter disk.\n",
    "read_bq_export_from_workspace_bucket <- function(export_path) {\n",
    "  col_types <- cols(standard_concept_name = col_character(), standard_concept_code = col_character(), standard_vocabulary = col_character(), measurement_type_concept_name = col_character(), operator_concept_name = col_character(), value_as_concept_name = col_character(), unit_concept_name = col_character(), visit_occurrence_concept_name = col_character(), measurement_source_value = col_character(), source_concept_name = col_character(), source_concept_code = col_character(), source_vocabulary = col_character(), unit_source_value = col_character(), value_source_value = col_character())\n",
    "  bind_rows(\n",
    "    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),\n",
    "        function(csv) {\n",
    "          message(str_glue('Loading {csv}.'))\n",
    "          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "          if (is.null(col_types)) {\n",
    "            col_types <- spec(chunk)\n",
    "          }\n",
    "          chunk\n",
    "        }))\n",
    "}\n",
    "dataset_69758583_measurement_df <- read_bq_export_from_workspace_bucket(measurement_69758583_path)\n",
    "\n",
    "dim(dataset_69758583_measurement_df)\n",
    "\n",
    "# head(dataset_69758583_measurement_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Glucose and HbA1c Measurements SQL - dataset_79242111_measurement_df\n",
    "library(tidyverse)\n",
    "library(bigrquery)\n",
    "\n",
    "# This query represents dataset \"Glucose and HbA1c Measures\" for domain \"measurement\" and was generated for All of Us Controlled Tier Dataset v7\n",
    "dataset_79242111_measurement_sql <- paste(\"\n",
    "    SELECT\n",
    "        measurement.person_id,\n",
    "        measurement.measurement_concept_id,\n",
    "        m_standard_concept.concept_name as standard_concept_name,\n",
    "        m_standard_concept.concept_code as standard_concept_code,\n",
    "        m_standard_concept.vocabulary_id as standard_vocabulary,\n",
    "        measurement.measurement_datetime,\n",
    "        measurement.measurement_type_concept_id,\n",
    "        m_type.concept_name as measurement_type_concept_name,\n",
    "        measurement.operator_concept_id,\n",
    "        m_operator.concept_name as operator_concept_name,\n",
    "        measurement.value_as_number,\n",
    "        measurement.value_as_concept_id,\n",
    "        m_value.concept_name as value_as_concept_name,\n",
    "        measurement.unit_concept_id,\n",
    "        m_unit.concept_name as unit_concept_name,\n",
    "        measurement.range_low,\n",
    "        measurement.range_high,\n",
    "        measurement.visit_occurrence_id,\n",
    "        m_visit.concept_name as visit_occurrence_concept_name,\n",
    "        measurement.measurement_source_value,\n",
    "        measurement.measurement_source_concept_id,\n",
    "        m_source_concept.concept_name as source_concept_name,\n",
    "        m_source_concept.concept_code as source_concept_code,\n",
    "        m_source_concept.vocabulary_id as source_vocabulary,\n",
    "        measurement.unit_source_value,\n",
    "        measurement.value_source_value \n",
    "    FROM\n",
    "        ( SELECT\n",
    "            * \n",
    "        FROM\n",
    "            `measurement` measurement \n",
    "        WHERE\n",
    "            (\n",
    "                measurement_concept_id IN (SELECT\n",
    "                    DISTINCT c.concept_id \n",
    "                FROM\n",
    "                    `cb_criteria` c \n",
    "                JOIN\n",
    "                    (SELECT\n",
    "                        CAST(cr.id as string) AS id       \n",
    "                    FROM\n",
    "                        `cb_criteria` cr       \n",
    "                    WHERE\n",
    "                        concept_id IN (3000483, 3003309, 3004410, 3004501, 3005673, 3007263, 3037110)       \n",
    "                        AND full_text LIKE '%_rank1]%'      ) a \n",
    "                        ON (c.path LIKE CONCAT('%.', a.id, '.%') \n",
    "                        OR c.path LIKE CONCAT('%.', a.id) \n",
    "                        OR c.path LIKE CONCAT(a.id, '.%') \n",
    "                        OR c.path = a.id) \n",
    "                WHERE\n",
    "                    is_standard = 1 \n",
    "                    AND is_selectable = 1)\n",
    "            )) measurement \n",
    "    LEFT JOIN\n",
    "        `concept` m_standard_concept \n",
    "            ON measurement.measurement_concept_id = m_standard_concept.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_type \n",
    "            ON measurement.measurement_type_concept_id = m_type.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_operator \n",
    "            ON measurement.operator_concept_id = m_operator.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_value \n",
    "            ON measurement.value_as_concept_id = m_value.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_unit \n",
    "            ON measurement.unit_concept_id = m_unit.concept_id \n",
    "    LEFT JOIn\n",
    "        `visit_occurrence` v \n",
    "            ON measurement.visit_occurrence_id = v.visit_occurrence_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_visit \n",
    "            ON v.visit_concept_id = m_visit.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_source_concept \n",
    "            ON measurement.measurement_source_concept_id = m_source_concept.concept_id\", sep=\"\")\n",
    "\n",
    "# Formulate a Cloud Storage destination path for the data exported from BigQuery.\n",
    "# NOTE: By default data exported multiple times on the same day will overwrite older copies.\n",
    "#       But data exported on a different days will write to a new location so that historical\n",
    "#       copies can be kept as the dataset definition is changed.\n",
    "measurement_79242111_path <- file.path(\n",
    "  Sys.getenv(\"WORKSPACE_BUCKET\"),\n",
    "  \"bq_exports\",\n",
    "  Sys.getenv(\"OWNER_EMAIL\"),\n",
    "  strftime(lubridate::now(), \"%Y%m%d\"),  # Comment out this line if you want the export to always overwrite.\n",
    "  \"measurement_79242111\",\n",
    "  \"measurement_79242111_*.csv\")\n",
    "message(str_glue('The data will be written to {measurement_79242111_path}. Use this path when reading ',\n",
    "                 'the data into your notebooks in the future.'))\n",
    "\n",
    "# Perform the query and export the dataset to Cloud Storage as CSV files.\n",
    "# NOTE: You only need to run `bq_table_save` once. After that, you can\n",
    "#       just read data from the CSVs in Cloud Storage.\n",
    "bq_table_save(\n",
    "  bq_dataset_query(Sys.getenv(\"WORKSPACE_CDR\"), dataset_79242111_measurement_sql, billing = Sys.getenv(\"GOOGLE_PROJECT\")),\n",
    "  measurement_79242111_path,\n",
    "  destination_format = \"CSV\")\n",
    "\n",
    "\n",
    "# Read the data directly from Cloud Storage into memory.\n",
    "# NOTE: Alternatively you can `gsutil -m cp {measurement_79242111_path}` to copy these files\n",
    "#       to the Jupyter disk.\n",
    "read_bq_export_from_workspace_bucket <- function(export_path) {\n",
    "  col_types <- cols(standard_concept_name = col_character(), standard_concept_code = col_character(), standard_vocabulary = col_character(), measurement_type_concept_name = col_character(), operator_concept_name = col_character(), value_as_concept_name = col_character(), unit_concept_name = col_character(), visit_occurrence_concept_name = col_character(), measurement_source_value = col_character(), source_concept_name = col_character(), source_concept_code = col_character(), source_vocabulary = col_character(), unit_source_value = col_character(), value_source_value = col_character())\n",
    "  bind_rows(\n",
    "    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),\n",
    "        function(csv) {\n",
    "          message(str_glue('Loading {csv}.'))\n",
    "          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "          if (is.null(col_types)) {\n",
    "            col_types <- spec(chunk)\n",
    "          }\n",
    "          chunk\n",
    "        }))\n",
    "}\n",
    "dataset_79242111_measurement_df <- read_bq_export_from_workspace_bucket(measurement_79242111_path)\n",
    "\n",
    "dim(dataset_79242111_measurement_df)\n",
    "\n",
    "# head(dataset_79242111_measurement_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Triglyceride Measures SQL - dataset_60405046_measurement_df\n",
    "library(tidyverse)\n",
    "library(bigrquery)\n",
    "\n",
    "# This query represents dataset \"Triglyceride Measures Dataset\" for domain \"measurement\" and was generated for All of Us Controlled Tier Dataset v7\n",
    "dataset_60405046_measurement_sql <- paste(\"\n",
    "    SELECT\n",
    "        measurement.person_id,\n",
    "        measurement.measurement_concept_id,\n",
    "        m_standard_concept.concept_name as standard_concept_name,\n",
    "        m_standard_concept.concept_code as standard_concept_code,\n",
    "        m_standard_concept.vocabulary_id as standard_vocabulary,\n",
    "        measurement.measurement_datetime,\n",
    "        measurement.measurement_type_concept_id,\n",
    "        m_type.concept_name as measurement_type_concept_name,\n",
    "        measurement.operator_concept_id,\n",
    "        m_operator.concept_name as operator_concept_name,\n",
    "        measurement.value_as_number,\n",
    "        measurement.value_as_concept_id,\n",
    "        m_value.concept_name as value_as_concept_name,\n",
    "        measurement.unit_concept_id,\n",
    "        m_unit.concept_name as unit_concept_name,\n",
    "        measurement.range_low,\n",
    "        measurement.range_high,\n",
    "        measurement.visit_occurrence_id,\n",
    "        m_visit.concept_name as visit_occurrence_concept_name,\n",
    "        measurement.measurement_source_value,\n",
    "        measurement.measurement_source_concept_id,\n",
    "        m_source_concept.concept_name as source_concept_name,\n",
    "        m_source_concept.concept_code as source_concept_code,\n",
    "        m_source_concept.vocabulary_id as source_vocabulary,\n",
    "        measurement.unit_source_value,\n",
    "        measurement.value_source_value \n",
    "    FROM\n",
    "        ( SELECT\n",
    "            * \n",
    "        FROM\n",
    "            `measurement` measurement \n",
    "        WHERE\n",
    "            (\n",
    "                measurement_concept_id IN (SELECT\n",
    "                    DISTINCT c.concept_id \n",
    "                FROM\n",
    "                    `cb_criteria` c \n",
    "                JOIN\n",
    "                    (SELECT\n",
    "                        CAST(cr.id as string) AS id       \n",
    "                    FROM\n",
    "                        `cb_criteria` cr       \n",
    "                    WHERE\n",
    "                        concept_id IN (3022038, 3022192)       \n",
    "                        AND full_text LIKE '%_rank1]%'      ) a \n",
    "                        ON (c.path LIKE CONCAT('%.', a.id, '.%') \n",
    "                        OR c.path LIKE CONCAT('%.', a.id) \n",
    "                        OR c.path LIKE CONCAT(a.id, '.%') \n",
    "                        OR c.path = a.id) \n",
    "                WHERE\n",
    "                    is_standard = 1 \n",
    "                    AND is_selectable = 1)\n",
    "            )) measurement \n",
    "    LEFT JOIN\n",
    "        `concept` m_standard_concept \n",
    "            ON measurement.measurement_concept_id = m_standard_concept.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_type \n",
    "            ON measurement.measurement_type_concept_id = m_type.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_operator \n",
    "            ON measurement.operator_concept_id = m_operator.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_value \n",
    "            ON measurement.value_as_concept_id = m_value.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_unit \n",
    "            ON measurement.unit_concept_id = m_unit.concept_id \n",
    "    LEFT JOIn\n",
    "        `visit_occurrence` v \n",
    "            ON measurement.visit_occurrence_id = v.visit_occurrence_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_visit \n",
    "            ON v.visit_concept_id = m_visit.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` m_source_concept \n",
    "            ON measurement.measurement_source_concept_id = m_source_concept.concept_id\", sep=\"\")\n",
    "\n",
    "# Formulate a Cloud Storage destination path for the data exported from BigQuery.\n",
    "# NOTE: By default data exported multiple times on the same day will overwrite older copies.\n",
    "#       But data exported on a different days will write to a new location so that historical\n",
    "#       copies can be kept as the dataset definition is changed.\n",
    "measurement_60405046_path <- file.path(\n",
    "  Sys.getenv(\"WORKSPACE_BUCKET\"),\n",
    "  \"bq_exports\",\n",
    "  Sys.getenv(\"OWNER_EMAIL\"),\n",
    "  strftime(lubridate::now(), \"%Y%m%d\"),  # Comment out this line if you want the export to always overwrite.\n",
    "  \"measurement_60405046\",\n",
    "  \"measurement_60405046_*.csv\")\n",
    "message(str_glue('The data will be written to {measurement_60405046_path}. Use this path when reading ',\n",
    "                 'the data into your notebooks in the future.'))\n",
    "\n",
    "# Perform the query and export the dataset to Cloud Storage as CSV files.\n",
    "# NOTE: You only need to run `bq_table_save` once. After that, you can\n",
    "#       just read data from the CSVs in Cloud Storage.\n",
    "bq_table_save(\n",
    "  bq_dataset_query(Sys.getenv(\"WORKSPACE_CDR\"), dataset_60405046_measurement_sql, billing = Sys.getenv(\"GOOGLE_PROJECT\")),\n",
    "  measurement_60405046_path,\n",
    "  destination_format = \"CSV\")\n",
    "\n",
    "\n",
    "# Read the data directly from Cloud Storage into memory.\n",
    "# NOTE: Alternatively you can `gsutil -m cp {measurement_60405046_path}` to copy these files\n",
    "#       to the Jupyter disk.\n",
    "read_bq_export_from_workspace_bucket <- function(export_path) {\n",
    "  col_types <- cols(standard_concept_name = col_character(), standard_concept_code = col_character(), standard_vocabulary = col_character(), measurement_type_concept_name = col_character(), operator_concept_name = col_character(), value_as_concept_name = col_character(), unit_concept_name = col_character(), visit_occurrence_concept_name = col_character(), measurement_source_value = col_character(), source_concept_name = col_character(), source_concept_code = col_character(), source_vocabulary = col_character(), unit_source_value = col_character(), value_source_value = col_character())\n",
    "  bind_rows(\n",
    "    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),\n",
    "        function(csv) {\n",
    "          message(str_glue('Loading {csv}.'))\n",
    "          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "          if (is.null(col_types)) {\n",
    "            col_types <- spec(chunk)\n",
    "          }\n",
    "          chunk\n",
    "        }))\n",
    "}\n",
    "dataset_60405046_measurement_df <- read_bq_export_from_workspace_bucket(measurement_60405046_path)\n",
    "\n",
    "dim(dataset_60405046_measurement_df)\n",
    "\n",
    "# head(dataset_60405046_measurement_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Get T2D measures and prediabetes status\n",
    "library(tidyverse)\n",
    "\n",
    "# Collect diabetes measures\n",
    "t2d_meas_tbl <- dataset_79242111_measurement_df %>% \n",
    "            select(person_id, measurement_concept_id, value_as_number, measurement_datetime)\n",
    "\n",
    "# Prediabetes status from labs\n",
    "library(sqldf)\n",
    "\n",
    "# Use SQL to collect measurements that meet prediabetes criteria\n",
    "prediabetes_meas_df <- sqldf(\"SELECT * FROM t2d_meas_tbl WHERE (measurement_concept_id IN ('3000483', '3004501') AND value_as_number <= 199 AND value_as_number >= 140) OR (measurement_concept_id = '3037110' AND value_as_number < 125 AND value_as_number >= 100) OR (measurement_concept_id IN ('3004410', '3007263', '3003309', '3005673') AND value_as_number >= 5.7 AND value_as_number <= 6.4)\")\n",
    "prediabetes_meas_df <- prediabetes_meas_df %>% \n",
    "    group_by(person_id) %>% \n",
    "    arrange(as.Date(measurement_datetime)) %>% \n",
    "    slice_head() \n",
    "\n",
    "dim(prediabetes_meas_df)\n",
    "# head(prediabetes_meas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Data frame of smoking start ages\n",
    "library(allofus)\n",
    "con <- aou_connect()\n",
    "\n",
    "smoking_obs_tbl <- tbl(con, \"observation\") |> filter(observation_concept_id == 40766333) |>\n",
    "            select(person_id, value_as_number) |> collect()\n",
    "\n",
    "# smoking_obs_tbl |> head()\n",
    "smoking_obs_tbl |> tally()\n",
    "\n",
    "# Convert person_id variable drawn in using the allofus package to numeric\n",
    "smoking_obs_tbl$person_id <- as.numeric(smoking_obs_tbl$person_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Pull in data frames from preparation step\n",
    "my_bucket <- Sys.getenv('WORKSPACE_BUCKET')\n",
    "\n",
    "system(paste0(\"gsutil cp \", my_bucket, \"/sid_pheno_files/\", \"statin_rx_df_v2_distinct.csv\", \" .\"), intern=T)\n",
    "statin_users_eof_df <- read.csv(\"statin_rx_df_v2_distinct.csv\")\n",
    "\n",
    "system(paste0(\"gsutil cp \", my_bucket, \"/sid_pheno_files/\", \"non_users_eof_df_v2.csv\", \" .\"), intern=T)\n",
    "non_users_eof_df <- read.csv(\"non_users_eof_df_v2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariates for Statin Users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: The purpose of this section is to assign covariate status at baseline (so most recent before statin initiation) for eligible statin users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Get necessary demographic info for statin users\n",
    "statin_user_dem_df <- inner_join(statin_users_eof_df, dataset_75324549_person_df, by = join_by(person_id)) %>% \n",
    "                        select(person_id, t2d_status, eof_age, eof_datetime, statin_init_age, statin_init_date, \n",
    "                               statin_type_start, statin_dose_start, statin_end_date, statin_end_age, \n",
    "                               statin_type_end, statin_dose_end, sex_at_birth, race, ethnicity)\n",
    "\n",
    "# Check that data frames joined correctly\n",
    "dim(statin_user_dem_df)\n",
    "# head(statin_user_dem_df)\n",
    "\n",
    "# Summarize demographic characteristics for current cohort\n",
    "# table(statin_user_dem_df$sex_at_birth)\n",
    "# table(statin_user_dem_df$race)\n",
    "# table(statin_user_dem_df$ethnicity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Create column for low HDL\n",
    "statin_user_hdl_df <- left_join(statin_user_dem_df, dataset_21028286_measurement_df) %>% \n",
    "                            mutate(low_hdl = case_when((sex_at_birth == \"Female\" & value_as_number <= 50) | \n",
    "                                                   (sex_at_birth == \"Male\" & value_as_number <= 40) ~ 1,\n",
    "                                                   (sex_at_birth == \"Female\" & value_as_number > 50) | \n",
    "                                                   (sex_at_birth == \"Male\" & value_as_number > 40)~ 0\n",
    "                                                   )) %>% \n",
    "                            group_by(person_id) %>% \n",
    "                            mutate(low_hdl = ifelse(any(low_hdl == 1), 1, 0)) %>% \n",
    "                            ungroup() %>%\n",
    "                            select(person_id, t2d_status, eof_age, eof_datetime, statin_init_age, statin_init_date, \n",
    "                                   statin_type_start, statin_dose_start, statin_end_date, statin_end_age, \n",
    "                                   statin_type_end, statin_dose_end, sex_at_birth, race, ethnicity, low_hdl) %>% \n",
    "                            distinct(.keep_all = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Create column for high TG\n",
    "statin_user_tg_df <- left_join(statin_user_hdl_df, dataset_60405046_measurement_df) %>% \n",
    "                            mutate(high_tg = ifelse(value_as_number >= 150 & as.Date(measurement_datetime) < statin_init_date, 1, 0)) %>% \n",
    "                            group_by(person_id) %>% \n",
    "                            mutate(tg_over_150 = ifelse(any(high_tg == 1), 1, 0)) %>% \n",
    "                            ungroup() %>%\n",
    "                            select(person_id, t2d_status, eof_age, eof_datetime, statin_init_age, statin_init_date, \n",
    "                                   statin_type_start, statin_dose_start, statin_end_date, statin_end_age, \n",
    "                                   statin_type_end, statin_dose_end, sex_at_birth, race, ethnicity, low_hdl,\n",
    "                                   tg_over_150) %>% \n",
    "                            distinct(.keep_all = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Create column for high BMI\n",
    "statin_user_bmi_df <- left_join(statin_user_tg_df, dataset_33462820_measurement_df) %>% \n",
    "                            mutate(high_bmi = ifelse(value_as_number >= 25 & as.Date(measurement_datetime) < statin_init_date, 1, 0)) %>% \n",
    "                            group_by(person_id) %>% \n",
    "                            mutate(bmi_over_25 = ifelse(any(high_bmi == 1), 1, 0)) %>% \n",
    "                            ungroup() %>%\n",
    "                            select(person_id, t2d_status, eof_age, eof_datetime, statin_init_age, statin_init_date, \n",
    "                                   statin_type_start, statin_dose_start, statin_end_date, statin_end_age, \n",
    "                                   statin_type_end, statin_dose_end, sex_at_birth, race, ethnicity, low_hdl,\n",
    "                                   tg_over_150, bmi_over_25) %>% \n",
    "                            distinct(.keep_all = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Create column for smoking status\n",
    "statin_user_smoking_df <- left_join(statin_user_bmi_df, smoking_obs_tbl) %>% \n",
    "                                mutate(smokingo_start_age = ifelse(person_id %in% smoking_obs_tbl$person_id, \n",
    "                                                               as.numeric(smoking_obs_tbl$value_as_number), NA),\n",
    "                                       smoking_status = ifelse(!is.na(smokingo_start_age) & smokingo_start_age <= statin_init_age, 1, 0)) %>%\n",
    "                                select(person_id, t2d_status, eof_age, eof_datetime, statin_init_age, statin_init_date, \n",
    "                                       statin_type_start, statin_dose_start, statin_end_date, statin_end_age, \n",
    "                                       statin_type_end, statin_dose_end, sex_at_birth, race, ethnicity, low_hdl,\n",
    "                                       tg_over_150, bmi_over_25, smoking_status)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Create column for HTN status\n",
    "statin_user_htn_icd_df <- left_join(statin_user_smoking_df, htn_icd_filtered_df) %>% \n",
    "                                mutate(htn_status = ifelse(!is.na(source_concept_code) & as.Date(condition_start_datetime) < statin_init_date, 1, 0)) %>%\n",
    "                                select(person_id, t2d_status, eof_age, eof_datetime, statin_init_age, statin_init_date, \n",
    "                                       statin_type_start, statin_dose_start, statin_end_date, statin_end_age, \n",
    "                                       statin_type_end, statin_dose_end, sex_at_birth, race, ethnicity, low_hdl, \n",
    "                                       tg_over_150, bmi_over_25, smoking_status, htn_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Create column for prediabetes based on lab values\n",
    "statin_user_pd_df <- left_join(statin_user_htn_icd_df, prediabetes_meas_df) %>% \n",
    "                            filter(person_id %in% statin_user_dem_df$person_id) %>% \n",
    "                            mutate(pd_status = ifelse(!is.na(measurement_concept_id) & \n",
    "                                   as.Date(measurement_datetime) < statin_init_date, 1, 0)) %>%\n",
    "                            select(person_id, t2d_status, eof_age, eof_datetime, statin_init_age, statin_init_date, \n",
    "                                   statin_type_start, statin_dose_start, statin_end_date, statin_end_age, \n",
    "                                   statin_type_end, statin_dose_end, sex_at_birth, race, ethnicity, low_hdl, \n",
    "                                   tg_over_150, bmi_over_25, smoking_status, htn_status, pd_status) %>%\n",
    "                            distinct(.keep_all = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Create column for gestational diabetes based on ICD codes\n",
    "statin_user_gd_df <- left_join(statin_user_pd_df, gd_code_df) %>%\n",
    "                            mutate(gd_status = ifelse(!is.na(standard_concept_code) & as.Date(condition_start_datetime) < statin_init_date, 1, 0)) %>%\n",
    "                            select(person_id, t2d_status, eof_age, eof_datetime, statin_init_age, statin_init_date, \n",
    "                                   statin_type_start, statin_dose_start, statin_end_date, statin_end_age, \n",
    "                                   statin_type_end, statin_dose_end, sex_at_birth, race, ethnicity, low_hdl, \n",
    "                                   tg_over_150, bmi_over_25, smoking_status, htn_status, pd_status, gd_status) %>%\n",
    "                            distinct(.keep_all = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Copy GD table to get final statin user with covariates table\n",
    "statin_user_covs_df <- statin_user_gd_df\n",
    "\n",
    "# Check final table\n",
    "length(unique(statin_user_covs_df$person_id))\n",
    "dim(statin_user_covs_df)\n",
    "# head(statin_user_covs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Summarize how many missing status' exist for each covariate\n",
    "# statin_user_covs_df %>% summarize(across(everything(), ~ sum(is.na(.x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Save statin user covariates into workspace bucket\n",
    "write.csv(statin_user_covs_df, \"statin_user_covs_df_v2.csv\")\n",
    "system(paste0(\"gsutil cp ./\", \"statin_user_covs_df_v2.csv\", \" \", my_bucket, \"/sid_pheno_files/\"), intern=T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariates for Non-users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: The purpose of this section is to assign statin non-users covariate statuses at every possible index date. Since the next step involves matching 2 non-users to each statin user, we want to create a larger pool of non-users to choose from by having multiple possible start dates for each non-user. Dates where non-users received fasting glucose, random glucose, or HbA1c measurements are considered eligible index dates, and thus, for non-users to be eligible for this study, they must have at least one of these measures.\n",
    "\n",
    "**Note**: Due to the large amount of data being handled in this section, the kernel is prone to dying. Data frames are saved at multiple checkpoints so that if kernel death does occur, analysis can be restarted at any of the checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Get T2D measures for eligible non-users since our index dates are based on T2D measures\n",
    "t2d_meas_tbl <- t2d_meas_tbl %>% filter(person_id %in% non_users_eof_df$person_id)\n",
    "\n",
    "# Get all random glucose/fasting glucose/HbA1c measures for non-users \n",
    "non_user_meas_df <- right_join(t2d_meas_tbl, non_users_eof_df, by = 'person_id')\n",
    "\n",
    "# Convert dates to characters for compatibility\n",
    "non_user_meas_df$eof_date <- as.character(non_user_meas_df$eof_datetime)\n",
    "non_user_meas_df$measurement_datetime <- as.character(non_user_meas_df$measurement_datetime)\n",
    "\n",
    "# Check starting non-user data frame\n",
    "length(unique(non_user_meas_df$person_id))\n",
    "dim(non_user_meas_df)\n",
    "# head(non_user_meas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Filter out index dates that occur after EoF or less than 30 days before EoF\n",
    "non_user_meas_filtered_df <- non_user_meas_df %>% filter(as.numeric(difftime(as.Date(eof_datetime), \n",
    "                                                                             as.Date(measurement_datetime),\n",
    "                                                                    units = 'days')) >= 30)\n",
    "\n",
    "# Check filtered non-user data frame\n",
    "length(unique(non_user_meas_filtered_df$person_id))\n",
    "dim(non_user_meas_filtered_df)\n",
    "# head(non_user_meas_filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Set seed to pick self-identified race, ethnicity, and sex at birth for non-users randomly since they \n",
    "# shouldn't change\n",
    "set.seed(32)\n",
    "# Get necessary demographic info for non users\n",
    "non_user_dem_df <- inner_join(non_user_meas_filtered_df, dataset_75324549_person_df, \n",
    "                              by = c(\"person_id\", \"date_of_birth\")) %>%\n",
    "                        mutate(index_id = sample(1:nrow(cur_data()), nrow(cur_data()))) %>%\n",
    "                        select(person_id, date_of_birth, t2d_status, eof_age, eof_datetime, sex_at_birth, \n",
    "                               race, ethnicity, measurement_concept_id, value_as_number, measurement_datetime, \n",
    "                               index_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Clean up column names and data types\n",
    "colnames(non_user_dem_df) <- c('person_id', 'date_of_birth', 't2d_status', 'eof_age', 'eof_datetime', \n",
    "                                    'sex_at_birth', 'race', 'ethnicity', 't2d_measurement_concept_id', \n",
    "                                    't2d_value_as_number', 't2d_measurement_index_date', 'index_id')\n",
    "\n",
    "non_user_dem_df$eof_date <- as.Date(non_user_dem_df$eof_datetime)\n",
    "non_user_dem_df$t2d_measurement_index_date <- as.Date(non_user_dem_df$t2d_measurement_index_date)\n",
    "dim(non_user_dem_df)\n",
    "# head(non_user_dem_df)\n",
    "\n",
    "# Save DF in case of kernel death\n",
    "write.csv(non_user_dem_df, \"non_user_dem_df_v2.csv\")\n",
    "system(paste0(\"gsutil cp ./\", \"non_user_dem_df_v2.csv\", \" \", my_bucket, \"/sid_pheno_files/covariate_helpers/\"), intern=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Create a skeleton data frame that only contains IDs and index dates\n",
    "helper_df <- non_user_dem_df %>%  \n",
    "                select(c('person_id', 't2d_measurement_index_date', 'index_id')) %>% distinct(.keep_all = TRUE)\n",
    "\n",
    "# Check helper df\n",
    "length(unique(helper_df$person_id))\n",
    "dim(helper_df)\n",
    "# head(helper_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Determining HDL status with joins - idea provided by lab members\n",
    "\n",
    "# Create a data frame with HDL measurements for all potential controls\n",
    "hdl_df <- dataset_21028286_measurement_df[dataset_21028286_measurement_df$person_id %in% \n",
    "                                               non_user_dem_df$person_id, c('person_id', 'value_as_number', \n",
    "                                                                                 'measurement_datetime')]\n",
    "\n",
    "# Determine low HDL status ahead of time\n",
    "hdl_df <- left_join(hdl_df, dataset_75324549_person_df) %>% \n",
    "                        select(person_id, value_as_number, measurement_datetime, sex_at_birth) %>%\n",
    "                        mutate(low_hdl = case_when((sex_at_birth == \"Female\" & value_as_number <= 50) | \n",
    "                                                   (sex_at_birth == \"Male\" & value_as_number <= 40) ~ 1,\n",
    "                                                   (sex_at_birth == \"Female\" & value_as_number > 50) | \n",
    "                                                   (sex_at_birth == \"Male\" & value_as_number > 40)~ 0\n",
    "                                                   )) %>%\n",
    "                        arrange(person_id, measurement_datetime) %>% group_by(person_id) %>% \n",
    "                        mutate(stretch_group = cumsum(low_hdl != lag(low_hdl, default = first(low_hdl)))) %>%\n",
    "                        group_by(person_id, low_hdl, stretch_group) %>%\n",
    "                        summarize(\n",
    "                            start_date = first(measurement_datetime),\n",
    "                            end_date = last(measurement_datetime),\n",
    "                            .groups = \"drop\"\n",
    "                                ) %>%\n",
    "                        select(person_id, low_hdl, start_date, end_date) %>% \n",
    "                        arrange(person_id, start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Join HDL and demographics data frames and group by person id and index date so that I can find the\n",
    "# most recent HDL measurement for each index date\n",
    "non_user_hdl_helper_df <- inner_join(helper_df, hdl_df, relationship = \"many-to-many\", by = \"person_id\") %>% \n",
    "                            group_by(person_id, t2d_measurement_index_date) %>% \n",
    "                            arrange(person_id, as.Date(t2d_measurement_index_date)) %>%\n",
    "                            filter(as.Date(start_date) <= as.Date(t2d_measurement_index_date)) %>%\n",
    "                            slice_tail()\n",
    "\n",
    "# Save DF in case of kernel death\n",
    "write.csv(non_user_hdl_helper_df, \"non_user_hdl_helper_df_v2.csv\")\n",
    "system(paste0(\"gsutil cp ./\", \"non_user_hdl_helper_df_v2.csv\", \" \", my_bucket, \"/sid_pheno_files/covariate_helpers/\"), intern=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Determining TG status with joins\n",
    "\n",
    "# Create a data frame with TG measurements for all potential controls\n",
    "tg_df <- dataset_60405046_measurement_df[dataset_60405046_measurement_df$person_id %in% \n",
    "                                         non_user_dem_df$person_id, c('person_id', 'value_as_number', \n",
    "                                                                      'measurement_datetime')]\n",
    "\n",
    "# Determine low TG status ahead of time\n",
    "tg_df <- tg_df %>% \n",
    "            mutate(high_tg = ifelse(value_as_number >= 150, 1, 0)) %>% \n",
    "            arrange(person_id, as.Date(measurement_datetime)) %>% group_by(person_id) %>% \n",
    "            mutate(stretch_group = cumsum(high_tg != lag(high_tg, default = first(high_tg)))) %>%\n",
    "            group_by(person_id, high_tg, stretch_group) %>%\n",
    "            summarize(\n",
    "                start_date = first(as.Date(measurement_datetime)),\n",
    "                end_date = last(measurement_datetime),\n",
    "                .groups = \"drop\"\n",
    "                    ) %>%\n",
    "            select(person_id, high_tg, start_date, end_date) %>% \n",
    "            arrange(person_id, start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Join TG and demographics data frames and group by person id and index date so that I can find the\n",
    "# most recent TG measurement for each index date\n",
    "non_user_tg_helper_df <- inner_join(helper_df, tg_df, relationship = \"many-to-many\", by = \"person_id\") %>% \n",
    "                            group_by(person_id, t2d_measurement_index_date) %>% \n",
    "                            arrange(person_id, t2d_measurement_index_date) %>%\n",
    "                            filter(start_date <= t2d_measurement_index_date) %>%\n",
    "                            slice_tail()\n",
    "\n",
    "# Save DF in case of kernel death\n",
    "write.csv(non_user_tg_helper_df, \"non_user_tg_helper_df_v2.csv\")\n",
    "system(paste0(\"gsutil cp ./\", \"non_user_tg_helper_df_v2.csv\", \" \", my_bucket, \"/sid_pheno_files/covariate_helpers/\"), intern=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Determining high BMI status with joins\n",
    "\n",
    "# Create a data frame with BMI measurements for all potential controls\n",
    "bmi_df <- dataset_33462820_measurement_df[dataset_33462820_measurement_df$person_id %in% \n",
    "                                          non_user_dem_df$person_id, c('person_id', 'value_as_number', \n",
    "                                                                       'measurement_datetime')]\n",
    "\n",
    "# Determine high BMI status ahead of time\n",
    "bmi_df <- bmi_df %>% \n",
    "                mutate(high_bmi = ifelse(value_as_number >= 25, 1, 0)) %>% \n",
    "                arrange(person_id, measurement_datetime) %>% group_by(person_id) %>% \n",
    "                mutate(stretch_group = cumsum(high_bmi != lag(high_bmi, default = first(high_bmi)))) %>%\n",
    "                group_by(person_id, high_bmi, stretch_group) %>%\n",
    "                summarize(\n",
    "                    start_date = first(measurement_datetime),\n",
    "                    end_date = last(measurement_datetime),\n",
    "                    .groups = \"drop\"\n",
    "                        ) %>%\n",
    "                select(person_id, high_bmi, start_date, end_date) %>% \n",
    "                arrange(person_id, start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Join BMI and demographics data frames and group by person id and index date so that I can find the\n",
    "# most recent BMI measurement for each index date\n",
    "non_user_bmi_helper_df <- inner_join(helper_df, bmi_df, relationship = \"many-to-many\", by = \"person_id\") %>% \n",
    "                            group_by(person_id, t2d_measurement_index_date) %>% \n",
    "                            arrange(person_id, t2d_measurement_index_date) %>%\n",
    "                            filter(start_date <= t2d_measurement_index_date) %>%\n",
    "                            slice_tail()\n",
    "\n",
    "# Save DF in case of kernel death\n",
    "write.csv(non_user_bmi_helper_df, \"non_user_bmi_helper_df_v2.csv\")\n",
    "system(paste0(\"gsutil cp ./\", \"non_user_bmi_helper_df_v2.csv\", \" \", my_bucket, \"/sid_pheno_files/covariate_helpers/\"), intern=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Create prediabetes helper table using SQL\n",
    "library(sqldf)\n",
    "non_user_dem_df <- non_user_dem_df %>% \n",
    "                    mutate(t2d_measurement_concept_id = as.numeric(t2d_measurement_concept_id))\n",
    "\n",
    "non_user_pd_helper_df <- sqldf(\"\n",
    "    SELECT \n",
    "        person_id, \n",
    "        t2d_measurement_concept_id, \n",
    "        t2d_value_as_number, \n",
    "        t2d_measurement_index_date, \n",
    "        index_id,\n",
    "        CASE \n",
    "            WHEN (t2d_measurement_concept_id IN ('3000483', '3004501') AND t2d_value_as_number <= 199 AND t2d_value_as_number >= 140) \n",
    "              OR (t2d_measurement_concept_id = '3037110' AND t2d_value_as_number < 125 AND t2d_value_as_number >= 100) \n",
    "              OR (t2d_measurement_concept_id IN ('3004410', '3007263', '3003309', '3005673') AND t2d_value_as_number >= 5.7 AND t2d_value_as_number <= 6.4) \n",
    "            THEN 1 \n",
    "            ELSE 0 \n",
    "        END AS pd_status \n",
    "    FROM \n",
    "        non_user_dem_df\n",
    "    \") %>% \n",
    "    select(person_id, t2d_measurement_index_date, index_id, pd_status) %>%\n",
    "    group_by(person_id, t2d_measurement_index_date) %>% \n",
    "    arrange(person_id, t2d_measurement_index_date)\n",
    "\n",
    "# Save DF in case of kernel death\n",
    "write.csv(non_user_pd_helper_df, \"non_user_pd_helper_df_v2.csv\")\n",
    "system(paste0(\"gsutil cp ./\", \"non_user_pd_helper_df_v2.csv\", \" \", my_bucket, \"/sid_pheno_files/covariate_helpers/\"), intern=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Determining smoking status\n",
    "\n",
    "# Create a data frame with smoking observations for all potential controls\n",
    "smoking_df <- smoking_obs_tbl[smoking_obs_tbl$person_id %in% non_user_dem_df$person_id, ] %>% \n",
    "                        rename(smoking_start_age = value_as_number) %>%\n",
    "                        mutate_at(vars(person_id), as.numeric)\n",
    "\n",
    "\n",
    "library(lubridate)\n",
    "# Find date of smoking start\n",
    "smoking_df <- full_join(non_user_dem_df, smoking_df) %>% \n",
    "                    mutate(smoking_start_date = as.Date(date_of_birth) %m+% years(smoking_start_age)) %>%\n",
    "                    select(person_id, smoking_start_date) %>% distinct(.keep_all = TRUE)\n",
    "\n",
    "# Find smoking status at each index date\n",
    "non_user_smoking_helper_df <- inner_join(helper_df, smoking_df) %>% \n",
    "                                    mutate(smoking_status = ifelse(!is.na(smoking_start_date) & \n",
    "                                                                   smoking_start_date <= t2d_measurement_index_date, \n",
    "                                                                   1, 0)) %>%\n",
    "                                    select(person_id, t2d_measurement_index_date, index_id, smoking_status)\n",
    "\n",
    "# Save DF in case of kernel death\n",
    "write.csv(non_user_smoking_helper_df, \"non_user_smoking_helper_df_v2.csv\")\n",
    "system(paste0(\"gsutil cp ./\", \"non_user_smoking_helper_df_v2.csv\", \" \", my_bucket, \"/sid_pheno_files/covariate_helpers/\"), intern=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Determining hypertension (HTN) status\n",
    "\n",
    "# Create a data frame with HTN ICD codes for all potential controls\n",
    "htn_df <- htn_icd_filtered_df[htn_icd_filtered_df$person_id %in% non_user_dem_df$person_id, ]\n",
    "\n",
    "# Find HTN status at each index date\n",
    "non_user_htn_helper_df <- full_join(helper_df, htn_df) %>% \n",
    "                            mutate(htn_status = ifelse(!is.na(condition_start_datetime) & \n",
    "                                                       as.Date(condition_start_datetime) <= t2d_measurement_index_date, \n",
    "                                                       1, 0)) %>%\n",
    "                            select(person_id, t2d_measurement_index_date, index_id, htn_status)\n",
    "\n",
    "# Save DF in case of kernel death\n",
    "write.csv(non_user_htn_helper_df, \"non_user_htn_helper_df_v2.csv\")\n",
    "system(paste0(\"gsutil cp ./\", \"non_user_htn_helper_df_v2.csv\", \" \", my_bucket, \"/sid_pheno_files/covariate_helpers/\"), intern=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Determining gestational diabetes (GD) status\n",
    "\n",
    "# Create a data frame with GD ICD codes for all potential controls\n",
    "gd_df <- gd_code_df[gd_code_df$person_id %in% non_user_dem_df$person_id, ]\n",
    "\n",
    "# Find GD status at each index date\n",
    "non_user_gd_helper_df <- full_join(helper_df, gd_df) %>% \n",
    "                            mutate(gd_status = ifelse(!is.na(condition_start_datetime) & \n",
    "                                                      as.Date(condition_start_datetime) <= t2d_measurement_index_date, \n",
    "                                                      1, 0)) %>%\n",
    "                            select(person_id, t2d_measurement_index_date, index_id, condition_start_datetime, \n",
    "                                   gd_status)\n",
    "\n",
    "# Save DF in case of kernel death\n",
    "write.csv(non_user_gd_helper_df, \"non_user_gd_helper_df_v2.csv\")\n",
    "system(paste0(\"gsutil cp ./\", \"non_user_gd_helper_df_v2.csv\", \" \", my_bucket, \"/sid_pheno_files/covariate_helpers/\"), intern=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(allofus)\n",
    "library(tidyverse)\n",
    "my_bucket <- Sys.getenv('WORKSPACE_BUCKET')\n",
    "\n",
    "# Reload data frames\n",
    "system(paste0(\"gsutil cp \", my_bucket, \"/sid_pheno_files/covariate_helpers/\", \"non_user_dem_df_v2.csv\", \" .\"), intern=T)\n",
    "non_user_dem_df <- read.csv(\"non_user_dem_df_v2.csv\")\n",
    "\n",
    "system(paste0(\"gsutil cp \", my_bucket, \"/sid_pheno_files/covariate_helpers/\", \"non_user_hdl_helper_df_v2.csv\", \" .\"), intern=T)\n",
    "non_user_hdl_helper_df <- read.csv(\"non_user_hdl_helper_df_v2.csv\")\n",
    "\n",
    "system(paste0(\"gsutil cp \", my_bucket, \"/sid_pheno_files/covariate_helpers/\", \"non_user_tg_helper_df_v2.csv\", \" .\"), intern=T)\n",
    "non_user_tg_helper_df <- read.csv(\"non_user_tg_helper_df_v2.csv\")\n",
    "\n",
    "system(paste0(\"gsutil cp \", my_bucket, \"/sid_pheno_files/covariate_helpers/\", \"non_user_bmi_helper_df_v2.csv\", \" .\"), intern=T)\n",
    "non_user_bmi_helper_df <- read.csv(\"non_user_bmi_helper_df_v2.csv\")\n",
    "\n",
    "system(paste0(\"gsutil cp \", my_bucket, \"/sid_pheno_files/covariate_helpers/\", \"non_user_pd_helper_df_v2.csv\", \" .\"), intern=T)\n",
    "non_user_pd_helper_df <- read.csv(\"non_user_pd_helper_df_v2.csv\")\n",
    "\n",
    "system(paste0(\"gsutil cp \", my_bucket, \"/sid_pheno_files/covariate_helpers/\", \"non_user_smoking_helper_df_v2.csv\", \" .\"), intern=T)\n",
    "non_user_smoking_helper_df <- read.csv(\"non_user_smoking_helper_df_v2.csv\")\n",
    "\n",
    "system(paste0(\"gsutil cp \", my_bucket, \"/sid_pheno_files/covariate_helpers/\", \"non_user_htn_helper_df_v2.csv\", \" .\"), intern=T)\n",
    "non_user_htn_helper_df <- read.csv(\"non_user_htn_helper_df_v2.csv\")\n",
    "\n",
    "system(paste0(\"gsutil cp \", my_bucket, \"/sid_pheno_files/covariate_helpers/\", \"non_user_gd_helper_df_v2.csv\", \" .\"), intern=T)\n",
    "non_user_gd_helper_df <- read.csv(\"non_user_gd_helper_df_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Check status counts for each covariate\n",
    "table(non_user_hdl_helper_df$low_hdl, useNA = 'always')\n",
    "table(non_user_bmi_helper_df$high_bmi, useNA = 'always')\n",
    "table(non_user_tg_helper_df$high_tg, useNA = 'always')\n",
    "table(non_user_pd_helper_df$pd_status, useNA = 'always')\n",
    "table(non_user_smoking_helper_df$smoking_status, useNA = 'always')\n",
    "table(non_user_htn_helper_df$htn_status, useNA = 'always')\n",
    "table(non_user_gd_helper_df$gd_status, useNA = 'always')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Check data frames\n",
    "# head(non_user_dem_df)\n",
    "# head(non_user_hdl_helper_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Combine helper data frames into one main data frame\n",
    "library(tidyverse)\n",
    "\n",
    "# HDL\n",
    "non_user_covs_df <- left_join(non_user_dem_df, non_user_hdl_helper_df, \n",
    "                              by = join_by(person_id, t2d_measurement_index_date, index_id)) %>% \n",
    "                        select(person_id, t2d_status, date_of_birth, eof_age, eof_date, sex_at_birth, race, \n",
    "                               ethnicity, t2d_measurement_concept_id, t2d_value_as_number, \n",
    "                               t2d_measurement_index_date, index_id, low_hdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# TG\n",
    "non_user_covs_df <- left_join(non_user_covs_df, non_user_tg_helper_df, \n",
    "                              by = join_by(person_id, t2d_measurement_index_date, index_id)) %>% \n",
    "                        select(person_id, t2d_status, date_of_birth, eof_age, eof_date, sex_at_birth, race, \n",
    "                               ethnicity, t2d_measurement_concept_id, t2d_value_as_number, \n",
    "                               t2d_measurement_index_date, index_id, low_hdl, high_tg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# BMI\n",
    "non_user_covs_df <- left_join(non_user_covs_df, non_user_bmi_helper_df, \n",
    "                              by = join_by(person_id, t2d_measurement_index_date, index_id)) %>% \n",
    "                        select(person_id, t2d_status, date_of_birth, eof_age, eof_date, sex_at_birth, race, \n",
    "                               ethnicity, t2d_measurement_concept_id, t2d_value_as_number, \n",
    "                               t2d_measurement_index_date, index_id, low_hdl, high_tg, high_bmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# PD status\n",
    "non_user_covs_df <- left_join(non_user_covs_df, non_user_pd_helper_df, \n",
    "                              by = join_by(person_id, t2d_measurement_index_date, index_id)) %>% \n",
    "                        select(person_id, t2d_status, date_of_birth, eof_age, eof_date, sex_at_birth, race, \n",
    "                               ethnicity, t2d_measurement_concept_id, t2d_value_as_number, \n",
    "                               t2d_measurement_index_date, index_id, low_hdl, high_tg, high_bmi, pd_status) %>% \n",
    "                        distinct(.keep_all = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Smoking status\n",
    "non_user_covs_df <- left_join(non_user_covs_df, non_user_smoking_helper_df, \n",
    "                              by = join_by(person_id, t2d_measurement_index_date, index_id)) %>% \n",
    "                        select(person_id, t2d_status, date_of_birth, eof_age, eof_date, sex_at_birth, race, \n",
    "                               ethnicity, t2d_measurement_concept_id, t2d_value_as_number, \n",
    "                               t2d_measurement_index_date, index_id, low_hdl, high_tg, high_bmi, pd_status, \n",
    "                               smoking_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# HTN status\n",
    "non_user_covs_df <- left_join(non_user_covs_df, non_user_htn_helper_df, \n",
    "                              by = join_by(person_id, t2d_measurement_index_date, index_id)) %>% \n",
    "                        select(person_id, t2d_status, date_of_birth, eof_age, eof_date, sex_at_birth, race, \n",
    "                               ethnicity, t2d_measurement_concept_id, t2d_value_as_number, \n",
    "                               t2d_measurement_index_date, index_id, low_hdl, high_tg, high_bmi, pd_status, \n",
    "                               smoking_status, htn_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# GD status\n",
    "non_user_covs_df <- left_join(non_user_covs_df, non_user_gd_helper_df, \n",
    "                              by = join_by(person_id, t2d_measurement_index_date, index_id)) %>% \n",
    "                        arrange(person_id, t2d_measurement_index_date) %>%\n",
    "                        select(person_id, t2d_status, date_of_birth, eof_age, eof_date, sex_at_birth, race, \n",
    "                               ethnicity, t2d_measurement_concept_id, t2d_value_as_number, \n",
    "                               t2d_measurement_index_date, index_id, low_hdl, high_tg, high_bmi, pd_status, \n",
    "                               smoking_status, htn_status, gd_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Check how much data is missing\n",
    "non_user_covs_df %>% summarize(na_low_hdl = sum(is.na(low_hdl)),\n",
    "                                         na_high_tg = sum(is.na(high_tg)),\n",
    "                                         na_high_bmi = sum(is.na(high_bmi)),\n",
    "                                         na_pd_status = sum(is.na(pd_status)),\n",
    "                                         na_smoking_status = sum(is.na(smoking_status)),\n",
    "                                         na_htn_status = sum(is.na(htn_status)),\n",
    "                                         na_gd_status = sum(is.na(gd_status)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Save non-user covariates df into workspace bucket\n",
    "write.csv(non_user_covs_df, \"non_users_covs_df_v2.csv\")\n",
    "system(paste0(\"gsutil cp ./\", \"non_users_covs_df_v2.csv\", \" \", my_bucket, \"/sid_pheno_files/\"), intern=T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
